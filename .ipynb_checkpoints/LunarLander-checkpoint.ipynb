{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Imports\n",
    "import gym\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from buffer import ReplayBuffer\n",
    "from collections import deque\n",
    "from gym import wrappers\n",
    "from networks import DQN\n",
    "from tqdm import tqdm\n",
    "from utils import set_seed_everywhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_replay():\n",
    "    \"\"\"\n",
    "    Not-so-elegant way to display the MP4 file generated by the Monitor wrapper inside a notebook.\n",
    "    The Monitor wrapper dumps the replay to a local file that we then display as a HTML video object.\n",
    "    \"\"\"\n",
    "    import io\n",
    "    import base64\n",
    "    from IPython.display import HTML\n",
    "    video = io.open('./gym-results/openaigym.video.%s.video000000.mp4' % env.file_infix, 'r+b').read()\n",
    "    encoded = base64.b64encode(video)\n",
    "    return HTML(data='''\n",
    "        <video width=\"360\" height=\"auto\" alt=\"test\" controls><source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" /></video>'''\n",
    "    .format(encoded.decode('ascii')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Hyperparameters\n",
    "NAME = 'dqn_32_clip_random_sgd'\n",
    "PATH_TRAINED = 'trained_models_ll/'\n",
    "PATH_STATS = 'statistics_ll/'\n",
    "\n",
    "general = {\n",
    "        \"seed\": 1,\n",
    "        \"checkpoint\": 50,\n",
    "        \"agent_type\": \"DQN\",\n",
    "        \"gradient_clipping\": True,\n",
    "        \"buffer_type\": \"Random\"\n",
    "    }\n",
    "\n",
    "params = {\n",
    "        \"n_episodes\": 1000,\n",
    "        \"batch_size\": 32,\n",
    "        \"buffer_cap\": 100000,\n",
    "        \"gamma\": 0.99,\n",
    "        \"tau\": 0.01\n",
    "    }\n",
    "\n",
    "nn_params ={\n",
    "        \"optimizer\": \"SGD\",\n",
    "        \"layers\": [64,64,32,16,8],\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"weight_decay\": 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Initialize environment\n",
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (nn): Sequential(\n",
       "    (layer_0): Linear(in_features=8, out_features=64, bias=True)\n",
       "    (act_0): ReLU()\n",
       "    (layer_1): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (act_1): ReLU()\n",
       "    (layer_2): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (act_2): ReLU()\n",
       "    (layer_3): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (act_3): ReLU()\n",
       "    (layer_4): Linear(in_features=16, out_features=8, bias=True)\n",
       "    (act_4): ReLU()\n",
       "    (layer_5): Linear(in_features=8, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%% Unpack parameters\n",
    "SEED = general['seed']\n",
    "AGENT_TYPE = general['agent_type']\n",
    "CHECKPOINT = general['checkpoint']\n",
    "CLAMP_GRAD = general['gradient_clipping']\n",
    "BUFFER_TYPE = general['buffer_type']\n",
    "N_EPISODES = params['n_episodes']\n",
    "BATCH_SIZE = params['batch_size']\n",
    "BUFFER_CAP = params['buffer_cap']\n",
    "GAMMA = params['gamma']\n",
    "TAU = params['tau']\n",
    "n_in = env.observation_space.shape[0]\n",
    "n_out = env.action_space.n\n",
    "\n",
    "# Fix NN layers\n",
    "layers = deque(nn_params['layers'])\n",
    "layers.appendleft(n_in)\n",
    "layers.append(n_out)\n",
    "nn_params['layers'] = layers\n",
    "\n",
    "# Instantiate networks and replay buffer\n",
    "policy_net = DQN(nn_params)\n",
    "target_net = DQN(nn_params)\n",
    "target_net.load_state_dict(policy_net.state_dict()) # same weights as policy_net\n",
    "if BUFFER_TYPE == \"Random\":\n",
    "    buffer = ReplayBuffer(BUFFER_CAP)\n",
    "\n",
    "# Move NN parameters to GPU (if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "policy_net.to(device)\n",
    "target_net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Fill experience replay buffer\n",
    "s = env.reset()\n",
    "while len(buffer) < BUFFER_CAP:\n",
    "    a = env.action_space.sample()\n",
    "    s1, r, done, info = env.step(a)\n",
    "    buffer.push(s, a, r, s1, done)\n",
    "    if not done:\n",
    "        s = np.copy(s1)\n",
    "    else:\n",
    "        s = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]<ipython-input-7-c8bbb069f927>:28: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  batch = np.array(buffer.sample(BATCH_SIZE))\n",
      "  5%|▌         | 50/1000 [00:35<10:16,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50:\n",
      "Epsilon: 0.9758055743720845\n",
      "Reward: -110.61636624761134\n",
      "Length: 84\n",
      "Losses: 564341.8421797946\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 100/1000 [01:08<10:02,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100:\n",
      "Epsilon: 0.9058021309857155\n",
      "Reward: -99.9601010933797\n",
      "Length: 149\n",
      "Losses: 904940.2629440746\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 150/1000 [01:57<16:28,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 150:\n",
      "Epsilon: 0.7998927863012315\n",
      "Reward: -89.24661562109104\n",
      "Length: 190\n",
      "Losses: 2857079.6650688993\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 200/1000 [02:48<13:29,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 200:\n",
      "Epsilon: 0.6720169513826998\n",
      "Reward: -120.81049884301696\n",
      "Length: 213\n",
      "Losses: 259518515.87271726\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 250/1000 [04:10<18:18,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 250:\n",
      "Epsilon: 0.5371557707427748\n",
      "Reward: -287.91904519316205\n",
      "Length: 118\n",
      "Losses: 579366330.0206673\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 300/1000 [05:13<11:30,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 300:\n",
      "Epsilon: 0.40852002917402486\n",
      "Reward: -308.5363476840229\n",
      "Length: 174\n",
      "Losses: 2134529672.474514\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 349/1000 [07:43<14:24,  1.33s/it]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c8bbb069f927>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;31m# Update network weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mCLAMP_GRAD\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/02456_l2rpn/networks.py\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, q_outputs, q_targets)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_targets\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mq_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TODO refractor code, no need to repeat training loop (adapt DQN to handle this)\n",
    "if AGENT_TYPE == \"DQN\":\n",
    "\n",
    "    epsilon = 1.0\n",
    "    rewards, lengths, losses, epsilons, dones = [], [], [], [], []\n",
    "    \n",
    "    for i in tqdm(range(N_EPISODES)):\n",
    "\n",
    "        length = 0\n",
    "        ep_reward = 0\n",
    "        ep_loss = 0\n",
    "        \n",
    "        s = env.reset()\n",
    "\n",
    "        while True:\n",
    "            \n",
    "            # Select action with epsilon-greedy strategy\n",
    "            if np.random.rand() < epsilon:\n",
    "                a = env.action_space.sample()\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    a = policy_net(torch.tensor(s, device=device).float()).argmax().item()\n",
    "\n",
    "            s1, r, done, info = env.step(a) # act\n",
    "            buffer.push(s, a, r, s1, done) # store transition in experience replay buffer\n",
    "\n",
    "            # Sample from buffer\n",
    "            batch = np.array(buffer.sample(BATCH_SIZE)) \n",
    "            s_batch, a_batch, r_batch, s1_batch , done_mask = batch[:,0], batch[:,1], batch[:,2], batch[:,3], batch[:, 4]\n",
    "\n",
    "            # Convert and to Tensor (if needed)\n",
    "            s_batch = torch.tensor(list(s_batch), device=device).float()\n",
    "            a_batch = torch.tensor(a_batch.astype(int, copy=False), device=device)\n",
    "            r_batch = torch.tensor(r_batch.astype(float, copy=False), device=device)\n",
    "            s1_batch = torch.tensor(list(s1_batch), device=device).float()\n",
    "            done_mask = done_mask.astype(bool, copy=False)\n",
    "\n",
    "            policy_net.optimizer.zero_grad() # clean gradients\n",
    "\n",
    "            # Compute policy Q-values (s, a) from observation batch\n",
    "            Q = policy_net(s_batch).gather(1, a_batch.unsqueeze(1))\n",
    "            \n",
    "            # Compute target max a Q(s', a) from next observation batch\n",
    "            Q1 = torch.zeros(BATCH_SIZE, device=device)\n",
    "            Q1[~done_mask] = target_net(s1_batch[~done_mask]).max(1)[0].detach()\n",
    "            \n",
    "            # Compute expected target values for each sampled experience\n",
    "            Q_target = r_batch + (GAMMA * Q1)\n",
    "\n",
    "            # Update network weights\n",
    "            loss = policy_net.loss(Q, Q_target.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            if CLAMP_GRAD:\n",
    "                for param in policy_net.parameters():\n",
    "                    param.grad.data.clamp_(-1, 1)\n",
    "            policy_net.optimizer.step()\n",
    "\n",
    "            # Update target network parameters from policy network parameters\n",
    "            target_net.update_params(policy_net.state_dict(), TAU)\n",
    "            \n",
    "            # Bookkeeping\n",
    "            s = np.copy(s1)\n",
    "            length += 1\n",
    "            ep_reward += r\n",
    "            ep_loss += loss.item()\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Bookkeeping\n",
    "        epsilon *= N_EPISODES/(i/(N_EPISODES/20) + N_EPISODES) # decrease epsilon\n",
    "        epsilons.append(epsilon); rewards.append(ep_reward); lengths.append(length); losses.append(ep_loss)\n",
    "\n",
    "        if (i+1) % CHECKPOINT == 0:\n",
    "            print(f\"Episode {i+1}:\\nEpsilon: {epsilon}\\nReward: {ep_reward}\\nLength: {length}\\nLosses: {ep_loss}\\n\")\n",
    "           \n",
    "    torch.save(policy_net.state_dict(), PATH_TRAINED + NAME + f'_policy_net_{i+1}.pth')\n",
    "    torch.save(target_net.state_dict(), PATH_TRAINED + NAME + f'_target_net_{i+1}.pth')\n",
    "    np.savez(PATH_STATS + NAME + f'_stats_{i+1}', epsilons=epsilons, rewards=rewards, lengths=lengths, losses=losses)\n",
    "\n",
    "elif AGENT_TYPE == \"DDQN\":\n",
    "    \n",
    "    epsilon = 1.0\n",
    "    rewards, lengths, losses, epsilons, dones = [], [], [], [], []\n",
    "    \n",
    "    for i in tqdm(range(N_EPISODES)):\n",
    "\n",
    "        length = 0\n",
    "        ep_reward = 0\n",
    "        ep_loss = 0\n",
    "        \n",
    "        s = env.reset()\n",
    "\n",
    "        while True:\n",
    "            \n",
    "            # Select action with epsilon-greedy strategy\n",
    "            if np.random.rand() < epsilon:\n",
    "                a = env.action_space.sample()\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    a = policy_net(torch.tensor(s, device=device).float()).argmax().item()\n",
    "\n",
    "            s1, r, done, info = env.step(a) # act\n",
    "            buffer.push(s, a, r, s1, done) # store transition in experience replay buffer\n",
    "\n",
    "            # Sample from buffer\n",
    "            batch = np.array(buffer.sample(BATCH_SIZE)) \n",
    "            s_batch, a_batch, r_batch, s1_batch , done_mask = batch[:,0], batch[:,1], batch[:,2], batch[:,3], batch[:, 4]\n",
    "\n",
    "            # Convert and to Tensor (if needed)\n",
    "            s_batch = torch.tensor(list(s_batch), device=device).float()\n",
    "            a_batch = torch.tensor(a_batch.astype(int, copy=False), device=device)\n",
    "            r_batch = torch.tensor(r_batch.astype(float, copy=False), device=device)\n",
    "            s1_batch = torch.tensor(list(s1_batch), device=device).float()\n",
    "            done_mask = done_mask.astype(bool, copy=False)\n",
    "\n",
    "            policy_net.optimizer.zero_grad() # clean gradients\n",
    "\n",
    "            # Compute Q-values (s, a) and a' from policy net\n",
    "            Q = policy_net(s_batch).gather(1, a_batch.unsqueeze(1))\n",
    "            a_next = policy_net(s1_batch[~done_mask]).argmax(dim=1, keepdim=True)\n",
    "            \n",
    "            # Compute Q-values (s', a') from target net\n",
    "            Q1 = torch.zeros(BATCH_SIZE, device=device)\n",
    "            Q1[~done_mask] = target_net(s1_batch[~done_mask]).gather(1, a_next).squeeze(1).detach()\n",
    "            \n",
    "            # Compute expected target values for each sampled experience\n",
    "            Q_target = r_batch + (GAMMA * Q1)\n",
    "\n",
    "            # Update network weights\n",
    "            loss = policy_net.loss(Q, Q_target.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            if CLAMP_GRAD:\n",
    "                for param in policy_net.parameters():\n",
    "                    param.grad.data.clamp_(-1, 1) # clamping gradients\n",
    "            policy_net.optimizer.step()\n",
    "\n",
    "            # Update target network parameters from policy network parameters\n",
    "            target_net.update_params(policy_net.state_dict(), TAU)\n",
    "            \n",
    "            # Bookkeeping\n",
    "            s = np.copy(s1)\n",
    "            length += 1\n",
    "            ep_reward += r\n",
    "            ep_loss += loss.item()\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Bookkeeping\n",
    "        epsilon *= N_EPISODES/(i/(N_EPISODES/20) + N_EPISODES) # decrease epsilon\n",
    "        epsilons.append(epsilon); rewards.append(ep_reward); lengths.append(length); losses.append(ep_loss)\n",
    "\n",
    "        if (i+1) % CHECKPOINT == 0:\n",
    "            print(f\"Episode {i+1}:\\nEpsilon: {epsilon}\\nReward: {ep_reward}\\nLength: {length}\\nLosses: {ep_loss}\\n\")\n",
    "           \n",
    "    torch.save(policy_net.state_dict(), PATH_TRAINED + NAME + f'_policy_net_{i+1}.pth')\n",
    "    torch.save(target_net.state_dict(), PATH_TRAINED + NAME + f'_target_net_{i+1}.pth')\n",
    "    np.savez(PATH_STATS + NAME + f'_stats_{i+1}', epsilons=epsilons, rewards=rewards, lengths=lengths, losses=losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <video width=\"360\" height=\"auto\" alt=\"test\" controls><source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" /></video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%% Random agent\n",
    "env = gym.make('LunarLander-v2')\n",
    "env = wrappers.Monitor(env, \"./gym-results\", force=True) \n",
    "env.reset()\n",
    "done = False\n",
    "while True:\n",
    "    env.render()\n",
    "    s, r, done, info = env.step(env.action_space.sample())\n",
    "    if done: break\n",
    "env.close()\n",
    "show_replay()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% DQN agent\n",
    "s = env.reset()\n",
    "done = False\n",
    "rewards = []\n",
    "while True:\n",
    "    env.render()\n",
    "    s, r, done, info = env.step(policy_net(torch.tensor(s, device=device).float()).argmax().item())\n",
    "    rewards.append(r)\n",
    "    if done: break\n",
    "env.close()\n",
    "show_replay()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rewards)\n",
    "plt.title(\"DQN agent\")\n",
    "plt.xlabel(\"Timestep\")\n",
    "plt.ylabel(\"Reward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% DDQN agent\n",
    "s = env.reset()\n",
    "done = False\n",
    "while True:\n",
    "    env.render()\n",
    "    s, r, done, info = env.step(policy_net(torch.tensor(s, device=device).float()).argmax().item())\n",
    "    if done: break\n",
    "env.close()\n",
    "show_replay()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rewards)\n",
    "plt.title(\"DDQN agent\")\n",
    "plt.xlabel(\"Timestep\")\n",
    "plt.ylabel(\"Reward\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actions:\n",
    "\n",
    "0. Do nothing\n",
    "1. Fire left engine\n",
    "2. Fire down engine\n",
    "3. Fire right engine"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
